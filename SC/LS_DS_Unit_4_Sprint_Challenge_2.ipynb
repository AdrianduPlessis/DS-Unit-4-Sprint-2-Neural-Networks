{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A single 'function' that given a set of inputs returns some output\n",
    "- **Input Layer:** Here each feature of the data is treated asif an output from some earlier layer, only the 'earlier layer' is the raw data.\n",
    "- **Hidden Layer:** The values being passed to this layer have already been abstracted from the raw data and is no longer obviously interpretable by a human.\n",
    "- **Output Layer:** This is the interpretable conclusion of the neural net, it's prediction.\n",
    "- **Activation:** If the sum of the weighted outputs from the earlier outputs meet the neuron's activation threshhold then it will fire causing its own weighted value to be passed forward.\n",
    "- **Backpropagation:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chocolate</th>\n      <th>gummy</th>\n      <th>ate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   chocolate  gummy  ate\n0          0      1    1\n1          1      0    1\n2          0      1    1\n3          0      0    0\n4          1      1    0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy[['ate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx / (1 - sx)\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.random((2,1))\n",
    "\n",
    "\n",
    "\n",
    "weighted_sum = np.dot(X, weights)\n",
    "\n",
    "# Output activated value (first iteration of predictions)\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "error = y - activated_output\n",
    "\n",
    "adjustments = error * sigmoid_derivative(activated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/adrian/.local/share/virtualenvs/DS-Unit-4-Sprint-2-Neural-Networks-pP9PLsrX/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n  This is separate from the ipykernel package so we can avoid doing imports until\nAcuracy after training: \n 0.3882\n"
    }
   ],
   "source": [
    "# Train Perceptron\n",
    "# Now, Update the weights n times, which should reduce the error\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "\n",
    "    # Activate\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "    # Calculate the error\n",
    "    error = y - activated_output\n",
    "\n",
    "    # Make adjustments informed by the error\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    # Update the weights\n",
    "    weights += np.dot(X.T, adjustments)\n",
    "\n",
    "accuracy = 1 - np.mean(np.abs(y - activated_output))\n",
    "print(\"Acuracy after training: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        X, input matrix\n",
    "        y, output matix\n",
    "        o, error\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------EPOCH 1---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.66585232]\n [0.69899744]\n [0.66585232]\n ...\n [0.66585232]\n [0.66585232]\n [0.69899744]]\nLoss: \n 0.28281336482874414\n+---------EPOCH 2---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.49987818]\n [0.49996918]\n [0.49987818]\n ...\n [0.49987818]\n [0.49987818]\n [0.49996918]]\nLoss: \n 0.20118402677336614\n+---------EPOCH 3---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 4---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 5---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n/home/adrian/.local/share/virtualenvs/DS-Unit-4-Sprint-2-Neural-Networks-pP9PLsrX/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n  app.launch_new_instance()\n+---------EPOCH 1000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 2000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 3000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 4000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.39048012646534114\n+---------EPOCH 5000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 6000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20114835473718157\n+---------EPOCH 7000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 8000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 9000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n+---------EPOCH 10000---------+\nInput: \n [[0 1]\n [1 0]\n [0 1]\n ...\n [0 1]\n [0 1]\n [1 0]]\nActual Output: \n [[1]\n [1]\n [1]\n ...\n [1]\n [1]\n [1]]\nPredicted Output: \n [[0.5]\n [0.5]\n [0.5]\n ...\n [0.5]\n [0.5]\n [0.5]]\nLoss: \n 0.20115\n"
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.5],\n       [0.5],\n       [0.5],\n       ...,\n       [0.5],\n       [0.5],\n       [0.5]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(303, 14)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>264</th>\n      <td>54</td>\n      <td>1</td>\n      <td>0</td>\n      <td>110</td>\n      <td>206</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>67</td>\n      <td>0</td>\n      <td>0</td>\n      <td>106</td>\n      <td>223</td>\n      <td>0</td>\n      <td>1</td>\n      <td>142</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>140</td>\n      <td>313</td>\n      <td>0</td>\n      <td>1</td>\n      <td>133</td>\n      <td>0</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>69</td>\n      <td>0</td>\n      <td>3</td>\n      <td>140</td>\n      <td>239</td>\n      <td>0</td>\n      <td>1</td>\n      <td>151</td>\n      <td>0</td>\n      <td>1.8</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>57</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>192</td>\n      <td>0</td>\n      <td>1</td>\n      <td>148</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n264   54    1   0       110   206    0        0      108      1      0.0   \n143   67    0   0       106   223    0        1      142      0      0.3   \n112   64    0   2       140   313    0        1      133      0      0.2   \n19    69    0   3       140   239    0        1      151      0      1.8   \n5     57    1   0       140   192    0        1      148      0      0.4   \n\n     slope  ca  thal  target  \n264      1   1     2       0  \n143      2   2     2       1  \n112      2   0     3       1  \n19       2   2     2       1  \n5        1   0     1       1  "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = df.drop(columns=\"target\").values\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = df[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 242 samples, validate on 61 samples\nEpoch 1/50\n242/242 [==============================] - 0s 2ms/sample - loss: 0.7070 - accuracy: 0.5579 - val_loss: 0.6304 - val_accuracy: 0.7213\nEpoch 2/50\n242/242 [==============================] - 0s 336us/sample - loss: 0.5974 - accuracy: 0.6901 - val_loss: 0.5615 - val_accuracy: 0.7869\nEpoch 3/50\n242/242 [==============================] - 0s 314us/sample - loss: 0.5332 - accuracy: 0.7727 - val_loss: 0.5093 - val_accuracy: 0.8033\nEpoch 4/50\n242/242 [==============================] - 0s 314us/sample - loss: 0.4853 - accuracy: 0.8058 - val_loss: 0.4694 - val_accuracy: 0.8033\nEpoch 5/50\n242/242 [==============================] - 0s 328us/sample - loss: 0.4471 - accuracy: 0.8182 - val_loss: 0.4392 - val_accuracy: 0.8033\nEpoch 6/50\n242/242 [==============================] - 0s 326us/sample - loss: 0.4158 - accuracy: 0.8264 - val_loss: 0.4137 - val_accuracy: 0.8197\nEpoch 7/50\n242/242 [==============================] - 0s 326us/sample - loss: 0.3917 - accuracy: 0.8306 - val_loss: 0.3960 - val_accuracy: 0.8197\nEpoch 8/50\n242/242 [==============================] - 0s 278us/sample - loss: 0.3723 - accuracy: 0.8347 - val_loss: 0.3842 - val_accuracy: 0.8361\nEpoch 9/50\n242/242 [==============================] - 0s 316us/sample - loss: 0.3565 - accuracy: 0.8388 - val_loss: 0.3750 - val_accuracy: 0.8361\nEpoch 10/50\n242/242 [==============================] - 0s 304us/sample - loss: 0.3431 - accuracy: 0.8430 - val_loss: 0.3688 - val_accuracy: 0.8361\nEpoch 11/50\n242/242 [==============================] - 0s 344us/sample - loss: 0.3345 - accuracy: 0.8471 - val_loss: 0.3639 - val_accuracy: 0.8361\nEpoch 12/50\n242/242 [==============================] - 0s 351us/sample - loss: 0.3258 - accuracy: 0.8512 - val_loss: 0.3605 - val_accuracy: 0.8361\nEpoch 13/50\n242/242 [==============================] - 0s 342us/sample - loss: 0.3186 - accuracy: 0.8636 - val_loss: 0.3588 - val_accuracy: 0.8361\nEpoch 14/50\n242/242 [==============================] - 0s 348us/sample - loss: 0.3110 - accuracy: 0.8554 - val_loss: 0.3597 - val_accuracy: 0.8361\nEpoch 15/50\n242/242 [==============================] - 0s 321us/sample - loss: 0.3042 - accuracy: 0.8719 - val_loss: 0.3620 - val_accuracy: 0.8525\nEpoch 16/50\n242/242 [==============================] - 0s 339us/sample - loss: 0.2977 - accuracy: 0.8802 - val_loss: 0.3631 - val_accuracy: 0.8525\nEpoch 17/50\n242/242 [==============================] - 0s 291us/sample - loss: 0.2927 - accuracy: 0.8760 - val_loss: 0.3658 - val_accuracy: 0.8361\nEpoch 18/50\n242/242 [==============================] - 0s 321us/sample - loss: 0.2879 - accuracy: 0.8719 - val_loss: 0.3672 - val_accuracy: 0.8361\nEpoch 19/50\n242/242 [==============================] - 0s 317us/sample - loss: 0.2836 - accuracy: 0.8802 - val_loss: 0.3670 - val_accuracy: 0.8361\nEpoch 20/50\n242/242 [==============================] - 0s 316us/sample - loss: 0.2775 - accuracy: 0.8802 - val_loss: 0.3694 - val_accuracy: 0.8361\nEpoch 21/50\n242/242 [==============================] - 0s 335us/sample - loss: 0.2729 - accuracy: 0.8843 - val_loss: 0.3723 - val_accuracy: 0.8361\nEpoch 22/50\n242/242 [==============================] - 0s 319us/sample - loss: 0.2714 - accuracy: 0.8760 - val_loss: 0.3695 - val_accuracy: 0.8361\nEpoch 23/50\n242/242 [==============================] - 0s 288us/sample - loss: 0.2628 - accuracy: 0.8967 - val_loss: 0.3737 - val_accuracy: 0.8197\nEpoch 24/50\n242/242 [==============================] - 0s 307us/sample - loss: 0.2584 - accuracy: 0.8926 - val_loss: 0.3765 - val_accuracy: 0.8197\nEpoch 25/50\n242/242 [==============================] - 0s 313us/sample - loss: 0.2536 - accuracy: 0.8884 - val_loss: 0.3826 - val_accuracy: 0.8197\nEpoch 26/50\n242/242 [==============================] - 0s 304us/sample - loss: 0.2476 - accuracy: 0.8884 - val_loss: 0.3805 - val_accuracy: 0.8197\nEpoch 27/50\n242/242 [==============================] - 0s 329us/sample - loss: 0.2433 - accuracy: 0.8967 - val_loss: 0.3838 - val_accuracy: 0.8197\nEpoch 28/50\n242/242 [==============================] - 0s 353us/sample - loss: 0.2385 - accuracy: 0.9008 - val_loss: 0.3870 - val_accuracy: 0.8197\nEpoch 29/50\n242/242 [==============================] - 0s 312us/sample - loss: 0.2351 - accuracy: 0.9008 - val_loss: 0.3854 - val_accuracy: 0.8197\nEpoch 30/50\n242/242 [==============================] - 0s 323us/sample - loss: 0.2312 - accuracy: 0.9050 - val_loss: 0.3903 - val_accuracy: 0.8033\nEpoch 31/50\n242/242 [==============================] - 0s 322us/sample - loss: 0.2277 - accuracy: 0.9091 - val_loss: 0.3931 - val_accuracy: 0.8033\nEpoch 32/50\n242/242 [==============================] - 0s 330us/sample - loss: 0.2227 - accuracy: 0.9091 - val_loss: 0.3894 - val_accuracy: 0.8033\nEpoch 33/50\n242/242 [==============================] - 0s 326us/sample - loss: 0.2200 - accuracy: 0.9091 - val_loss: 0.3936 - val_accuracy: 0.8033\nEpoch 34/50\n242/242 [==============================] - 0s 296us/sample - loss: 0.2182 - accuracy: 0.9174 - val_loss: 0.3927 - val_accuracy: 0.8033\nEpoch 35/50\n242/242 [==============================] - 0s 271us/sample - loss: 0.2125 - accuracy: 0.9174 - val_loss: 0.4008 - val_accuracy: 0.8197\nEpoch 36/50\n242/242 [==============================] - 0s 312us/sample - loss: 0.2093 - accuracy: 0.9215 - val_loss: 0.4003 - val_accuracy: 0.8033\nEpoch 37/50\n242/242 [==============================] - 0s 330us/sample - loss: 0.2053 - accuracy: 0.9298 - val_loss: 0.4024 - val_accuracy: 0.8197\nEpoch 38/50\n242/242 [==============================] - 0s 346us/sample - loss: 0.2014 - accuracy: 0.9256 - val_loss: 0.4076 - val_accuracy: 0.8033\nEpoch 39/50\n242/242 [==============================] - 0s 316us/sample - loss: 0.1983 - accuracy: 0.9298 - val_loss: 0.4112 - val_accuracy: 0.8197\nEpoch 40/50\n242/242 [==============================] - 0s 328us/sample - loss: 0.1946 - accuracy: 0.9339 - val_loss: 0.4203 - val_accuracy: 0.8033\nEpoch 41/50\n242/242 [==============================] - 0s 292us/sample - loss: 0.1928 - accuracy: 0.9256 - val_loss: 0.4263 - val_accuracy: 0.7869\nEpoch 42/50\n242/242 [==============================] - 0s 335us/sample - loss: 0.1880 - accuracy: 0.9380 - val_loss: 0.4253 - val_accuracy: 0.8197\nEpoch 43/50\n242/242 [==============================] - 0s 310us/sample - loss: 0.1846 - accuracy: 0.9380 - val_loss: 0.4333 - val_accuracy: 0.8033\nEpoch 44/50\n242/242 [==============================] - 0s 320us/sample - loss: 0.1818 - accuracy: 0.9421 - val_loss: 0.4378 - val_accuracy: 0.7869\nEpoch 45/50\n242/242 [==============================] - 0s 325us/sample - loss: 0.1796 - accuracy: 0.9380 - val_loss: 0.4412 - val_accuracy: 0.7869\nEpoch 46/50\n242/242 [==============================] - 0s 331us/sample - loss: 0.1747 - accuracy: 0.9380 - val_loss: 0.4397 - val_accuracy: 0.7869\nEpoch 47/50\n242/242 [==============================] - 0s 323us/sample - loss: 0.1719 - accuracy: 0.9421 - val_loss: 0.4407 - val_accuracy: 0.7869\nEpoch 48/50\n242/242 [==============================] - 0s 316us/sample - loss: 0.1704 - accuracy: 0.9463 - val_loss: 0.4524 - val_accuracy: 0.7541\nEpoch 49/50\n242/242 [==============================] - 0s 314us/sample - loss: 0.1653 - accuracy: 0.9463 - val_loss: 0.4566 - val_accuracy: 0.7541\nEpoch 50/50\n242/242 [==============================] - 0s 338us/sample - loss: 0.1624 - accuracy: 0.9380 - val_loss: 0.4559 - val_accuracy: 0.7705\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fe89c3206d0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = X_train.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test,y_test), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          # For ease of grading\n",
    "          verbose=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.818367350101471 using {'batch_size': 10, 'epochs': 20}\nMeans: 0.818367350101471, Stdev: 0.02927123513828241 with: {'batch_size': 10, 'epochs': 20}\nMeans: 0.8059523940086365, Stdev: 0.032449815235266084 with: {'batch_size': 20, 'epochs': 20}\nMeans: 0.7686224460601807, Stdev: 0.03281188206591915 with: {'batch_size': 40, 'epochs': 20}\nMeans: 0.7688775539398194, Stdev: 0.036053416227158915 with: {'batch_size': 60, 'epochs': 20}\nMeans: 0.669132649898529, Stdev: 0.05870328380394499 with: {'batch_size': 80, 'epochs': 20}\nMeans: 0.7151360630989074, Stdev: 0.07725143296555516 with: {'batch_size': 100, 'epochs': 20}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.7811224460601807 using {'batch_size': 10, 'epochs': 20}\nMeans: 0.7811224460601807, Stdev: 0.027004693827132498 with: {'batch_size': 10, 'epochs': 20}\nMeans: 0.7728741526603699, Stdev: 0.05801188858662839 with: {'batch_size': 20, 'epochs': 20}\nMeans: 0.7727891206741333, Stdev: 0.04327772201819599 with: {'batch_size': 40, 'epochs': 20}\nMeans: 0.7442176938056946, Stdev: 0.061892027801758294 with: {'batch_size': 60, 'epochs': 20}\nMeans: 0.7357142925262451, Stdev: 0.0623604973678458 with: {'batch_size': 80, 'epochs': 20}\nMeans: 0.7193877577781678, Stdev: 0.06579511394909926 with: {'batch_size': 100, 'epochs': 20}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.7938775539398193 using {'batch_size': 20, 'epochs': 20}\nMeans: 0.7934523820877075, Stdev: 0.03436721736080849 with: {'batch_size': 10, 'epochs': 20}\nMeans: 0.7938775539398193, Stdev: 0.051723748513328636 with: {'batch_size': 20, 'epochs': 20}\nMeans: 0.7937925219535827, Stdev: 0.08386396458468011 with: {'batch_size': 40, 'epochs': 20}\nMeans: 0.7521258473396302, Stdev: 0.03884854054289091 with: {'batch_size': 60, 'epochs': 20}\nMeans: 0.7399659872055053, Stdev: 0.030951194159139148 with: {'batch_size': 80, 'epochs': 20}\nMeans: 0.6369897902011872, Stdev: 0.14375940056923764 with: {'batch_size': 100, 'epochs': 20}\n"
    }
   ],
   "source": [
    "Adding an additional hidden layer resulted in lower accuracy of 0.79\n",
    "\n",
    "Reverting to single hidden layer and testing different activation functions. Keeping Best 'batch_size' of 20 and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.8431972861289978 using {'batch_size': 7, 'epochs': 20}\nMeans: 0.826955771446228, Stdev: 0.05756494465359904 with: {'batch_size': 5, 'epochs': 20}\nMeans: 0.8431972861289978, Stdev: 0.029773307080887948 with: {'batch_size': 7, 'epochs': 20}\nMeans: 0.8267857193946838, Stdev: 0.04683868818642726 with: {'batch_size': 9, 'epochs': 20}\nMeans: 0.7810374140739441, Stdev: 0.04041934800643247 with: {'batch_size': 11, 'epochs': 20}\n"
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [5, 7, 9, 11],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best batch size determined to be x: 5<x<9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.8185374140739441 using {'batch_size': 8, 'epochs': 20}\nMeans: 0.8142006874084473, Stdev: 0.033476852743194896 with: {'batch_size': 6, 'epochs': 20}\nMeans: 0.7854591846466065, Stdev: 0.04807963341270975 with: {'batch_size': 7, 'epochs': 20}\nMeans: 0.8185374140739441, Stdev: 0.053428442694877495 with: {'batch_size': 8, 'epochs': 20}\n"
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [6,7,8],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy lower than previous fit, but 8 consistantly wins out as best batch size.\n",
    "Testing alternative activation function then concluding with Grid CV of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.818367338180542 using {'batch_size': 8, 'epochs': 80}\nMeans: 0.8141156435012817, Stdev: 0.03440611883195407 with: {'batch_size': 8, 'epochs': 20}\nMeans: 0.7852891206741333, Stdev: 0.019124152431700438 with: {'batch_size': 8, 'epochs': 40}\nMeans: 0.818367338180542, Stdev: 0.02927122904326694 with: {'batch_size': 8, 'epochs': 80}\nMeans: 0.7893707513809204, Stdev: 0.048782845522695385 with: {'batch_size': 8, 'epochs': 160}\n"
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [8],\n",
    "              'epochs': [20, 40, 80, 160]}\n",
    "              \n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Nueral Networks (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}